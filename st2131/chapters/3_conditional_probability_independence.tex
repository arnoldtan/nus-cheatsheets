\documentclass[../st2131_notes.tex]{subfiles}
\begin{document}

\chapter{Conditional Probability and Independence}
\section{Conditional Probability}

\subsection{Definition of Conditional Probability}
\textbf{Definition.} Let \(E\) and \(F\) be two events. Suppose that \(P(F)>0\), the conditional probability of \(E\) given \(F\), denoted by \(P(E\vert F)\), is defined as
\[P(E\vert F)=\frac{P(EF)}{P(F)}\]

\subsubsection{Remark}
\(P(E\vert F)\) can also be read as: the conditional probability that \(E\) occurs given that \(F\) has occurred.

\subsection{Multiplication Rule}
Suppose that \(P(A)>0\), then
\[P(AB)=P(A)P(B\vert A)\]

\subsection{General Multiplication Rule}
Let \(A_1,A_2,\ldots,A_n\) be \(n\) events, then
\[P(A_1A_2\ldots A_n)=P(A_1)P(A_2\vert A_1)P(A_3\vert A_1A_2)\ldots P(A_n\vert A_1A_2\ldots A_{n-1})\]

\section{Bayes' Formulas}
\subsection{Partition of a Sample Space}
\(A_1,A_2,\ldots,A_n\) \textbf{partition} the \textbf{sample space} \(S\) if
\begin{itemize}
	\item They are ``mutually exclusive'', i.e. \(A_i\cap A_j=\varnothing\), for all \(i\ne j\); and
	\item They are ``collectively exhaustive'', i.e. \(\bigcup\limits_{i=1}^nA_i=S\).
\end{itemize}

\subsection{Bayes' First Formula / Law of Total Probability}
\textbf{Proposition.} Suppose the events \(A_1,A_2,\ldots,A_n\) \textbf{partition} the \textbf{sample space}. Assume further that \(P(A_i)>0\) for \(1\leq i\leq n\). Let \(B\) be any event, then
\[P(B)=P(B\vert A_1)P(A_1)+P(B\vert A_2)P(A_2)+\ldots+P(B\vert A_n)P(A_n)\]

\subsubsection{Special Case: Partition into $A$ and $A^c$}
Let \(A\) and \(B\) be any two events. Then the events \(A,A^c\) partition the \textbf{sample space}. Hence,
\[P(B)=P(B\vert A)P(A)+P(B\vert A^c)P(A^c)\]

\subsubsection{Remark}
\begin{itemize}
	\item This formula is useful in computing the probability of a composite event, i.e. an event \(B\) which depends on a series of causes \(A_1,A_2,\ldots,A_n\).
	\item The summation can be interpreted as a weighted average of \(n\) cases: case \(1\) being \(A_1\) occurs, case \(2\) being \(A_2\) occurs, \(\ldots\), case \(n\) being \(A_n\) occurs.
	\item The weights are placed according to how likely \(A_1,A_2,\ldots,A_n\) occur.
\end{itemize}

\subsection{Bayes' Second Formula}
Suppose the events \(A_1,A_2,\ldots,A_n\) \textbf{partition} the \textbf{sample space}. Assume further that \(P(A_i)>0\) for \(1\leq i\leq n\). Let \(B\) be any event. Then for \(1\leq i\leq n\),
\[P(A_i\vert B)=\frac{P(B\vert A_i)P(A_i)}{P(B\vert A_1)P(A_1)+P(B\vert A_2)P(A_2)+\ldots+P(B\vert A_n)P(A_n)}\]

\subsubsection{Remarks}
\begin{itemize}
	\item This formula has been interpreted as a formula for ``inverse probabilities''.
	\item If \(A_1,A_2,\ldots,A_n\) is a series of causes and \(B\) is a possible effect, then
	\begin{itemize}
		\item\(P(B\vert A_i)\) is the probability of \(B\) when it is known that \(A_i\) is the cause, whereas
		\item\(P(A_i\vert B)\) is the probability that \(A_i\) is the cause when it is known that \(B\) is the effect.
	\end{itemize}
\end{itemize}

\subsection{Odds of an Event}
\textbf{Definition.} The \textbf{odds} of an event \(A\) is defined by
\[\frac{P(A)}{P(A^c)}=\frac{P(A)}{1-P(A)}\]

\section{Independent Events}

\subsection{Definition of Two Independent / Dependent Events}
\textbf{Definition.} Two events \(A\) and \(B\) are said to be \textbf{independent} if
\[P(AB)=P(A)P(B)\]
and they are said to be \textbf{dependent} if
\[P(AB)\ne P(A)P(B)\]

\subsubsection{Remarks}
The following phrases mean the same:
\begin{itemize}
	\item\(A\) and \(B\) are independent;
	\item\(A\) is independent of \(B\);
	\item\(B\) is independent of \(A\).
\end{itemize}

\subsection{Independent Events and their Conditional Probabilities}
If two events \(A\) and \(B\) are \textbf{independent}. Suppose \(P(B)>0\), then
\begin{align*}
	P(A\vert B)&=\frac{P(AB)}{P(B)} \\
	&=\frac{P(A)P(B)}{P(B)} \\
	&=P(A)
\end{align*}
Similarly, if \(P(A)>0\),
\[P(B\vert A)=P(B)\]

\subsection{Independent Events and their Complement Events}
\textbf{Proposition.} If \(A\) and \(B\) are \textbf{independent}, then so are
\begin{itemize}
	\item\(A\) and \(B^c\);
	\item\(A^c\) and \(B\);
	\item\(A^c\) and \(B^c\).
\end{itemize}

\subsection{Event is Not Always Independent of the Product of Events it is Independent to}
If \(A\) is independent of \(B\), and \(A\) is also independent of \(C\), it is NOT necessarily true that \(A\) is independent of \(BC\).

\subsection{Definition of Three Independent Events}
\textbf{Definition.} Three events \(A\), \(B\), and \(C\) are said to be independent if all of the following 4 conditions hold:
\begin{enumerate}
	\item\(P(ABC)=P(A)P(B)P(C)\)
	\item\(P(AB)=P(A)P(B)\)
	\item\(P(AC)=P(A)P(C)\)
	\item\(P(BC)=P(B)P(C)\)
\end{enumerate}

\subsubsection{Remarks}
The second to fourth conditions mean that \(A\), \(B\), and \(C\) are pairwise independent, i.e.:
\begin{itemize}
	\item\(A\) and \(B\) are independent (from second condition)
	\item\(A\) and \(C\) are independent (from third condition)
	\item\(B\) and \(C\) are independent (from fourth condition)
\end{itemize}

\subsection{Definition of $n$ Independent Events}
\textbf{Definition.} Events \(A_1,A_2,\ldots,A_n\) are \textbf{independent} if, for every sub-collection of events \(A_{i_1},A_{i_2},\ldots,A_{i_r}\),
\[P(A_{i_1}A_{i_2}\cdots A_{i_r})=P(A_{i_1})P(A_{i_2})\cdots P(A_{i_r})\]

\section{Algebra of Conditional Probability}
\textbf{Proposition.} Let \(A\) be an event with \(P(A)>0\). Then the following three conditions hold:
\begin{enumerate}
	\item For any event \(B\),
	\[0\leq P(B\vert A)\leq1\]
	\item\(P(S\vert A)=1\)
	\item Let \(B_1,B_2,\ldots\) be a sequence of mutually exclusive events, then
	\[P\left(\bigcup_{k=1}^\infty B_k\middle\vert A\right)=\sum_{k=1}^\infty P(B_k\vert A)\]
\end{enumerate}

\subsection{Remarks}
Therefore, \(P(\cdot\vert A)\) as a function of events satisfy the three axioms of probability.
\end{document}